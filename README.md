# PhysioTouch - 触觉感知预训练模型

## 项目概述

PhysioTouch是一个基于MAE（Masked Autoencoder）架构的触觉感知预训练模型项目，专门用于处理触觉传感器数据。该项目实现了对触觉视频序列的自监督学习，能够从触觉传感器数据中学习有效的表示。

## 核心功能

### 1. 触觉视频MAE模型 (TactileVideoMAE)
- **架构**: 基于CLIP-ViT-L-14预训练模型，结合MAE自监督学习
- **输入**: 连续4帧触觉传感器图像序列 (224×224×3)
- **掩码策略**: 随机掩码75%的图像块进行重建任务
- **传感器支持**: 支持多种触觉传感器类型（Digit、DuraGel、GelSight等）

### 2. 数据处理
- **数据集**: Tacquad触觉数据集
- **数据格式**: CSV文件包含文件夹名称、起始帧、结束帧信息
- **数据增强**: 随机翻转、颜色抖动等增强策略
- **序列处理**: 连续4帧触觉图像作为输入序列

### 3. 训练策略
- **自监督学习**: 通过掩码重建任务学习触觉表示
- **预训练权重**: 从CLIP视觉模型初始化触觉编码器
- **混合精度训练**: 支持FP16训练加速
- **分布式训练**: 支持多GPU并行训练

## 项目结构

```
PhysioTouch/
├── main_stage1.py          # 主训练脚本
├── stage1_engine.py        # 训练引擎
├── config.py               # 配置参数
├── model/
│   ├── mae_model.py        # MAE模型定义
│   ├── multi_model.py      # 多模态模型
│   └── process_clip.py     # CLIP处理工具
├── dataloader/
│   └── stage1_dataset.py   # 数据加载器
├── util/                   # 工具函数
├── Tacquad.csv            # 数据集索引文件
└── State1_training.sh     # 训练启动脚本
```

## 使用方法

### 1. 环境配置
```bash
# 安装依赖
pip install torch torchvision transformers timm einops
```

### 2. 数据准备
- 将触觉数据集放置在指定目录
- 更新`Tacquad.csv`文件中的路径信息
- 确保数据格式为连续帧的PNG图像

### 3. 模型训练
```bash
# 使用提供的训练脚本
bash State1_training.sh

# 或直接运行Python脚本
python main_stage1.py \
    --batch_size 8 \
    --epochs 2 \
    --output_dir ./log/test \
    --use_video
```

### 4. 主要参数说明
- `--batch_size`: 批次大小
- `--epochs`: 训练轮数
- `--mask_ratio`: 掩码比例（默认0.75）
- `--use_video`: 启用视频模式
- `--use_sensor_token`: 启用传感器令牌
- `--output_dir`: 输出目录

## 技术特点

### 1. 模型架构
- **编码器**: 基于CLIP-ViT-L-14的视觉变换器
- **解码器**: 轻量级ViT解码器用于重建
- **位置编码**: 2D正弦余弦位置编码
- **掩码策略**: 随机掩码75%的图像块

### 2. 训练优化
- **学习率调度**: 余弦退火学习率
- **权重衰减**: AdamW优化器
- **梯度累积**: 支持大批次训练
- **混合精度**: FP16训练加速

### 3. 数据处理
- **多传感器支持**: Digit、DuraGel、GelSight等
- **时序建模**: 4帧连续触觉图像
- **数据增强**: 随机翻转、颜色抖动
- **标准化**: ImageNet标准化参数

## 应用场景

1. **触觉感知研究**: 机器人触觉感知能力提升
2. **触觉数据理解**: 触觉传感器数据的表示学习
3. **下游任务**: 触觉分类、触觉导航等任务的基础模型
4. **多模态学习**: 视觉-触觉多模态表示学习

## 注意事项

1. **数据路径**: 确保CSV文件中的路径正确
2. **GPU内存**: 建议使用至少8GB显存的GPU
3. **数据格式**: 输入图像应为RGB格式的PNG文件
4. **序列长度**: 当前支持4帧连续序列

## 未来改进

1. **多传感器融合**: 支持更多类型的触觉传感器
2. **时序建模**: 增强长序列时序建模能力
3. **下游任务**: 添加触觉分类、检测等下游任务
4. **模型压缩**: 支持模型量化和压缩

## 许可证

本项目采用MIT许可证，详见LICENSE文件。
