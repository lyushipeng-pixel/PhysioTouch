import sys

import copy

import torch
import numpy as np
from einops import rearrange
from typing import Optional, Tuple, Union

from torch import nn
from transformers import CLIPModel as HFCLIPModel, CLIPVisionConfig
from transformers import AutoConfig
from transformers.modeling_outputs import BaseModelOutputWithPooling
from transformers.models.clip.modeling_clip import CLIP_VISION_INPUTS_DOCSTRING,CLIPVisionTransformer
from transformers.models.vit.modeling_vit import ViTLayer
from transformers.utils import replace_return_docstrings, add_start_docstrings_to_model_forward
from model.process_clip import get_global_value, set_global_value
from model.util.pos_embed import get_2d_sincos_pos_embed

class TactileMAE(nn.Module):
    def __init__(self, args, config, decoder_config, num_frames, add_time_attn, tube_size):
        super(TactileMAE, self).__init__()

        config.vision_config.num_frames = num_frames
        config.vision_config.tube_size = tube_size

        self.use_sensor_token = args.use_sensor_token
        self.new_decoder_sensor_token = args.new_decoder_sensor_token

        self.norm_pix_loss = False
        if args.norm_pix_loss:
            self.norm_pix_loss = args.norm_pix_loss

        self.touch_model = CLIPVisionTransformer(config.vision_config)
        self.touch_projection = nn.Linear(config.vision_config.hidden_size, config.projection_dim, bias=False)
        
        self.decoder_embed = nn.Linear(config.projection_dim, decoder_config.hidden_size, bias=True)
        self.num_patches = self.touch_model.embeddings.num_patches
        self.patch_size = config.vision_config.patch_size
        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, decoder_config.hidden_size), requires_grad=False)
        self.touch_decoder_blocks = nn.ModuleList([ViTLayer(decoder_config) for _ in range(decoder_config.num_hidden_layers)])

        self.decoder_norm = nn.LayerNorm(decoder_config.hidden_size, eps=decoder_config.layer_norm_eps)
        self.decoder_pred = nn.Linear(decoder_config.hidden_size, decoder_config.patch_size**2 * decoder_config.num_channels, bias=True)
        
        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_config.hidden_size))

        if self.use_sensor_token:
            self.sensor_token = nn.Parameter(torch.zeros(10, 5, config.vision_config.hidden_size))
            if self.new_decoder_sensor_token:
                self.sensor_token_proj = nn.Linear(config.vision_config.hidden_size, decoder_config.hidden_size, bias=False)

        self.mask_ratio = args.mask_ratio

        self.touch_model.forward = self.touch_forward
        self.touch_model.embeddings.forward = self.emb_forward

    
    def initialize_decoder(self):
        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], int(self.num_patches**.5), cls_token=True)
        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))

        torch.nn.init.normal_(self.mask_token, std=.02)
        if self.use_sensor_token:
            torch.nn.init.normal_(self.sensor_token, std=.02)

    def random_masking(self, sequence, noise=None):
        """
        Perform per-sample random masking by per-sample shuffling. Per-sample shuffling is done by argsort random
        noise.

        Args:
            sequence (`torch.LongTensor` of shape `(batch_size, sequence_length, dim)`)
            noise (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) which is
                mainly used for testing purposes to control randomness and maintain the reproducibility
        """
        batch_size, seq_length, dim = sequence.shape
        len_keep = int(seq_length * (1 - self.mask_ratio))

        if noise is None:
            noise = torch.rand(batch_size, seq_length, device=sequence.device)  # noise in [0, 1]

        # sort noise for each sample
        ids_shuffle = torch.argsort(noise, dim=1).to(sequence.device)  # ascend: small is keep, large is remove
        ids_restore = torch.argsort(ids_shuffle, dim=1).to(sequence.device)

        # keep the first subset
        ids_keep = ids_shuffle[:, :len_keep]
        sequence_unmasked = torch.gather(sequence, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, dim))

        # generate the binary mask: 0 is keep, 1 is remove
        mask = torch.ones([batch_size, seq_length], device=sequence.device)
        mask[:, :len_keep] = 0
        # unshuffle to get the binary mask
        mask = torch.gather(mask, dim=1, index=ids_restore)

        return sequence_unmasked, mask, ids_restore

    @add_start_docstrings_to_model_forward(CLIP_VISION_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=CLIPVisionConfig)
    def touch_forward(
        self,
        pixel_values: Optional[torch.FloatTensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        sensor_type = None
    ) -> Union[Tuple, BaseModelOutputWithPooling]:
        r"""
        Returns:

        """
        # a = self.sensor_token[sensor_type]
        # print(a.shape)
        output_attentions = output_attentions if output_attentions is not None else self.touch_model.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.touch_model.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.touch_model.config.use_return_dict

        if pixel_values is None:
            raise ValueError("You have to specify pixel_values")

        hidden_states, mask, ids_restore  = self.touch_model.embeddings(pixel_values, sensor_type = sensor_type)
        hidden_states = self.touch_model.pre_layrnorm(hidden_states)

        encoder_outputs = self.touch_model.encoder(
            inputs_embeds=hidden_states,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        last_hidden_state = encoder_outputs[0]
        pooled_output = last_hidden_state[:, 0, :]
        pooled_output = self.touch_model.post_layernorm(pooled_output)

        if not return_dict:
            return (last_hidden_state, pooled_output) + encoder_outputs[1:]

        return BaseModelOutputWithPooling(
            last_hidden_state=last_hidden_state,
            pooler_output=pooled_output,
            hidden_states=encoder_outputs.hidden_states,
            attentions=encoder_outputs.attentions,
        ), mask, ids_restore

    def emb_forward(self, pixel_values: torch.FloatTensor, noise=None, sensor_type=None) -> torch.Tensor:
        batch_size = pixel_values.shape[0]
        target_dtype = self.touch_model.embeddings.patch_embedding.weight.dtype
        patch_embeds = self.touch_model.embeddings.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]
        patch_embeds = patch_embeds.flatten(2).transpose(1, 2)

        pos_emb = self.touch_model.embeddings.position_embedding(self.touch_model.embeddings.position_ids)

        embeddings = patch_embeds + pos_emb[:, 1:, :]
        embeddings, mask, ids_restore = self.random_masking(embeddings, noise)

        class_embeds = self.touch_model.embeddings.class_embedding + pos_emb[:, 0, :]
        class_embeds = class_embeds.expand(batch_size, 1, -1)

        if self.use_sensor_token:
            sensor_emb = self.sensor_token[sensor_type]
            embeddings = torch.cat([class_embeds, sensor_emb, embeddings], dim=1)
        else:
            embeddings = torch.cat([class_embeds, embeddings], dim=1)
        #embeddings = embeddings + self.position_embedding(self.position_ids)
        return embeddings, mask, ids_restore

    def forward(self, x, sensor_type=None, target_sensor_type=None):
        latent, mask, ids_restore = self.forward_encoder(x, sensor_type=sensor_type)
        pred = self.forward_decoder(latent, ids_restore, sensor_type=target_sensor_type)

        loss = self.forward_loss(x, pred, mask)
        
        return loss, pred, mask

    def forward_encoder(self, x, sensor_type=None):

        x, mask, ids_restore = self.touch_model(x, sensor_type=sensor_type)
        out = self.touch_projection(x.last_hidden_state)

        return out, mask, ids_restore
    
    def forward_decoder(self, x, ids_restore, sensor_type=None):

        x = self.decoder_embed(x)

        if self.use_sensor_token:
            mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 6 - x.shape[1], 1)

            x_ = torch.cat([x[:, 6:, :], mask_tokens], dim=1)  # no cls token and sensor token
            x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle
            if self.new_decoder_sensor_token:
                decoder_sensor = self.sensor_token_proj(self.sensor_token[sensor_type])
                x = torch.cat([x[:, :1, :], decoder_sensor, x_], dim=1)  # append cls token and sensor token
            else:
                x = torch.cat([x[:, :6, :], x_], dim=1)  # append cls token and sensor token

            x[:,0,:] += self.decoder_pos_embed[:,0,:]
            x[:,6:,:] += self.decoder_pos_embed[:,1:,:]
        else:
            mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)

            x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token
            x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle
            x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token

            x = x + self.decoder_pos_embed

        for blk in self.touch_decoder_blocks:
            layer_outputs = blk(x)
        
            x = layer_outputs[0]

        x = self.decoder_norm(x)

        x = self.decoder_pred(x)

        if self.use_sensor_token:
            x = x[:, 6:, :]
        else:
            x = x[:, 1:, :]

        return x
    
    def forward_loss(self, x, pred, mask):
        target = self.patchify(x)

        if self.norm_pix_loss:
            mean = target.mean(dim=-1, keepdim=True)
            var = target.var(dim=-1, keepdim=True)
            target = (target - mean) / (var + 1.e-6)**.5

        loss = (pred - target) ** 2
        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch

        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches
        return loss

    def patchify(self, imgs):
        """
        imgs: (N, 3, H, W)
        x: (N, L, patch_size**2 *3)
        """
        p = self.patch_size
        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0

        h = w = imgs.shape[2] // p
        x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))
        x = torch.einsum('nchpwq->nhwpqc', x)
        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))
        return x

    def unpatchify(self, x):
        """
        x: (N, L, patch_size**2 *3)
        imgs: (N, 3, H, W)
        """
        p = self.patch_size
        h = w = int(x.shape[1]**.5)
        assert h * w == x.shape[1]
        
        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))
        x = torch.einsum('nhwpqc->nchpwq', x)
        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))
        return imgs

class TactileVideoMAE(nn.Module):
    def __init__(self, args, config, decoder_config, num_frames, add_time_attn, tube_size):
        super(TactileVideoMAE, self).__init__()

        config.vision_config.num_frames = num_frames
        config.vision_config.tube_size = tube_size

        self.use_sensor_token = args.use_sensor_token
        self.use_same_patchemb = args.use_same_patchemb
        self.new_decoder_sensor_token = args.new_decoder_sensor_token

        self.norm_pix_loss = False
        if args.norm_pix_loss:
            self.norm_pix_loss = args.norm_pix_loss

        self.touch_model = CLIPVisionTransformer(config.vision_config)
        self.touch_projection = nn.Linear(config.vision_config.hidden_size, config.projection_dim, bias=False)

        self.video_patch_embedding = nn.Conv3d(
            in_channels=config.vision_config.num_channels,
            out_channels=self.touch_model.embeddings.embed_dim,
            kernel_size=(3, self.touch_model.embeddings.patch_size, self.touch_model.embeddings.patch_size),
            stride=(3, self.touch_model.embeddings.patch_size, self.touch_model.embeddings.patch_size),
            bias=False,
        )
        self.video_position_embedding = nn.Embedding(self.touch_model.embeddings.num_positions, self.touch_model.embeddings.embed_dim)
        
        self.decoder_embed = nn.Linear(config.projection_dim, decoder_config.hidden_size, bias=True)
        self.num_patches = self.touch_model.embeddings.num_patches
        self.patch_size = config.vision_config.patch_size
        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, decoder_config.hidden_size), requires_grad=False)
        self.touch_decoder_blocks = nn.ModuleList([ViTLayer(decoder_config) for _ in range(decoder_config.num_hidden_layers)])

        self.decoder_norm = nn.LayerNorm(decoder_config.hidden_size, eps=decoder_config.layer_norm_eps)
        self.decoder_pred = nn.Linear(decoder_config.hidden_size, decoder_config.patch_size**2 * decoder_config.num_channels, bias=True)

        self.decoder_pred_video = nn.Linear(decoder_config.hidden_size, decoder_config.patch_size**2 * decoder_config.num_channels * 4, bias=True)
        
        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_config.hidden_size))

        if self.use_sensor_token:
            self.sensor_token = nn.Parameter(torch.zeros(10, 5, config.vision_config.hidden_size))
            self.beta = 1.0
            if self.new_decoder_sensor_token:
                self.sensor_token_proj = nn.Linear(config.vision_config.hidden_size, decoder_config.hidden_size, bias=False)

        self.mask_ratio = args.mask_ratio

        self.touch_model.forward = self.touch_forward
        self.touch_model.embeddings.forward = self.emb_forward

    
    def initialize_decoder(self):
        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], int(self.num_patches**.5), cls_token=True)
        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))

        torch.nn.init.normal_(self.mask_token, std=.02)
        if self.use_sensor_token:
            torch.nn.init.normal_(self.sensor_token, std=.02)

    def random_masking(self, sequence, noise=None):
        """
        Perform per-sample random masking by per-sample shuffling. Per-sample shuffling is done by argsort random
        noise.

        Args:
            sequence (`torch.LongTensor` of shape `(batch_size, sequence_length, dim)`)
            noise (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) which is
                mainly used for testing purposes to control randomness and maintain the reproducibility
        """
        batch_size, seq_length, dim = sequence.shape
        len_keep = int(seq_length * (1 - self.mask_ratio))

        if noise is None:
            noise = torch.rand(batch_size, seq_length, device=sequence.device)  # noise in [0, 1]

        # sort noise for each sample
        ids_shuffle = torch.argsort(noise, dim=1).to(sequence.device)  # ascend: small is keep, large is remove
        ids_restore = torch.argsort(ids_shuffle, dim=1).to(sequence.device)

        # keep the first subset
        ids_keep = ids_shuffle[:, :len_keep]
        sequence_unmasked = torch.gather(sequence, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, dim))

        # generate the binary mask: 0 is keep, 1 is remove
        mask = torch.ones([batch_size, seq_length], device=sequence.device)
        mask[:, :len_keep] = 0
        # unshuffle to get the binary mask
        mask = torch.gather(mask, dim=1, index=ids_restore)

        return sequence_unmasked, mask, ids_restore

    @add_start_docstrings_to_model_forward(CLIP_VISION_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=CLIPVisionConfig)
    def touch_forward(
        self,
        pixel_values: Optional[torch.FloatTensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        sensor_type = None,
        data_type = None,
        use_mask = True
    ) -> Union[Tuple, BaseModelOutputWithPooling]:
        r"""
        Returns:

        """
        # a = self.sensor_token[sensor_type]
        # print(a.shape)
        output_attentions = output_attentions if output_attentions is not None else self.touch_model.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.touch_model.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.touch_model.config.use_return_dict

        if pixel_values is None:
            raise ValueError("You have to specify pixel_values")

        hidden_states, mask, ids_restore  = self.touch_model.embeddings(pixel_values, sensor_type = sensor_type, data_type = data_type, use_mask = use_mask)
        hidden_states = self.touch_model.pre_layrnorm(hidden_states)

        encoder_outputs = self.touch_model.encoder(
            inputs_embeds=hidden_states,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        last_hidden_state = encoder_outputs[0]
        pooled_output = last_hidden_state[:, 0, :]
        pooled_output = self.touch_model.post_layernorm(pooled_output)

        if not return_dict:
            return (last_hidden_state, pooled_output) + encoder_outputs[1:]

        return BaseModelOutputWithPooling(
            last_hidden_state=last_hidden_state,
            pooler_output=pooled_output,
            hidden_states=encoder_outputs.hidden_states,
            attentions=encoder_outputs.attentions,
        ), mask, ids_restore

    def emb_forward(self, pixel_values: torch.FloatTensor, noise=None, sensor_type=None, data_type = None, use_mask = True) -> torch.Tensor:
        batch_size = pixel_values.shape[0]
        target_dtype = self.touch_model.embeddings.patch_embedding.weight.dtype
        if data_type == 0 and (not self.use_same_patchemb):
            patch_embeds = self.touch_model.embeddings.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]
        else:
            patch_embeds = self.video_patch_embedding(pixel_values.to(dtype=target_dtype))

        patch_embeds = patch_embeds.flatten(2).transpose(1, 2)

        pos_emb = self.touch_model.embeddings.position_embedding(self.touch_model.embeddings.position_ids)

        embeddings = patch_embeds + pos_emb[:, 1:, :]
        if use_mask:
            embeddings, mask, ids_restore = self.random_masking(embeddings, noise)
        else:
            mask = torch.ones(1)
            ids_restore = torch.ones(1)

        class_embeds = self.touch_model.embeddings.class_embedding + pos_emb[:, 0, :]
        class_embeds = class_embeds.expand(batch_size, 1, -1)

        if self.use_sensor_token:
            sensor_emb = self.sensor_token[sensor_type]
            embeddings = torch.cat([class_embeds, sensor_emb, embeddings], dim=1)
        else:
            embeddings = torch.cat([class_embeds, embeddings], dim=1)
        #embeddings = embeddings + self.position_embedding(self.position_ids)
        return embeddings, mask, ids_restore

    def forward(self, x, sensor_type=None, data_type=None, target_sensor_type = None):
        if data_type == 0:
            latent, mask, ids_restore = self.forward_encoder(x, sensor_type=sensor_type, data_type=data_type)
        else:
            latent, mask, ids_restore = self.forward_encoder(x[:, :3], sensor_type=sensor_type, data_type=data_type)
        pred = self.forward_decoder(latent, ids_restore, data_type=data_type, sensor_type=target_sensor_type)

        loss = self.forward_loss(x, pred, mask, data_type=data_type)
        
        return loss, pred, mask

    def forward_encoder(self, x, sensor_type=None, data_type = None, use_mask = True):
        if data_type == 0 and self.use_same_patchemb:
            x = x.unsqueeze(1).repeat(1, 3, 1, 1, 1)
            # print(x.shape)
        x, mask, ids_restore = self.touch_model(x, sensor_type=sensor_type, data_type=data_type, use_mask = use_mask)
        if use_mask:
            out = self.touch_projection(x.last_hidden_state)
        else:
            out = self.touch_projection(x.pooler_output)

        return out, mask, ids_restore
    
    def forward_decoder(self, x, ids_restore, sensor_type=None, data_type = None):

        x = self.decoder_embed(x)

        if self.use_sensor_token:
            mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 6 - x.shape[1], 1)

            x_ = torch.cat([x[:, 6:, :], mask_tokens], dim=1)  # no cls token and sensor token
            x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle

            if self.new_decoder_sensor_token:
                decoder_sensor = self.sensor_token_proj(self.sensor_token[sensor_type])
                x = torch.cat([x[:, :1, :], decoder_sensor, x_], dim=1)  # append cls token and sensor token
            else:
                x = torch.cat([x[:, :6, :], x_], dim=1)  # append cls token and sensor token
            #x = torch.cat([x[:, :6, :], x_], dim=1)  # append cls token and sensor token

            x[:,0,:] += self.decoder_pos_embed[:,0,:]
            x[:,6:,:] += self.decoder_pos_embed[:,1:,:]
        else:
            mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)

            x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token
            x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle
            x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token

            x = x + self.decoder_pos_embed

        for blk in self.touch_decoder_blocks:
            layer_outputs = blk(x)
        
            x = layer_outputs[0]

        x = self.decoder_norm(x)

        if data_type == 0:
            x = self.decoder_pred(x)
        else:
            x = self.decoder_pred_video(x)

        if self.use_sensor_token:
            x = x[:, 6:, :]
        else:
            x = x[:, 1:, :]

        if data_type == 1:
            x = x.view(x.shape[0], x.shape[1], 4, -1)
        return x
    
    def forward_loss(self, x, pred, mask, data_type = None):
        target = self.patchify(x, data_type = data_type)

        if self.norm_pix_loss:
            mean = target.mean(dim=-1, keepdim=True)
            var = target.var(dim=-1, keepdim=True)
            target = (target - mean) / (var + 1.e-6)**.5
        
        if data_type == 0:
            loss = (pred - target) ** 2
            loss = loss.mean(dim=-1)  # [N, L], mean loss per patch

            loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches
        else:
            loss = (pred - target) ** 2
            loss_pred = loss[:, :, 3, :].mean(dim=-1)
            loss_recon = loss[:, :, :3, :].mean(dim=-2).mean(dim=-1)
            L = loss.shape[1] * loss.shape[0]
            loss = (loss_recon * mask).sum() / mask.sum() + loss_pred.sum() / L
        
        return loss

    def patchify(self, imgs, data_type = None):
        """
        imgs: (N, 3, H, W)
        x: (N, L, patch_size**2 *3)
        """
        if data_type == 0:
            # image
            p = self.patch_size
            assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0

            h = w = imgs.shape[2] // p
            x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))
            x = torch.einsum('nchpwq->nhwpqc', x)
            x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))
        else:
            # video
            p = self.patch_size
            assert imgs.shape[3] == imgs.shape[4] and imgs.shape[3] % p == 0

            h = w = imgs.shape[3] // p
            x = imgs.reshape(shape=(imgs.shape[0], 4, 3, h, p, w, p))
            x = torch.einsum('ntchpwq->nhwtpqc', x)
            x = x.reshape(shape=(imgs.shape[0], h * w, 4, p**2 * 3))
        return x

    def unpatchify(self, x, data_type = None):
        """
        x: (N, L, patch_size**2 *3)
        imgs: (N, 3, H, W)
        """
        if data_type == 0:
            # image
            p = self.patch_size
            h = w = int(x.shape[1]**.5)
            assert h * w == x.shape[1]
            
            x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))
            x = torch.einsum('nhwpqc->nchpwq', x)
            imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))

        else:
            p = self.patch_size
            h = w = int(x.shape[1]**.5)
            assert h * w == x.shape[1]

            x = x.reshape(shape=(x.shape[0], h, w, 4, p, p, 3))
            x = torch.einsum('nhwtpqc->ntchpwq', x)
            imgs = x.reshape(shape=(x.shape[0], 4, 3, h * p, h * p))
        return imgs