"""
stage1_engine_static.py - æ”¯æŒé™æ€é‡å»ºæŸå¤±çš„è®­ç»ƒå¼•æ“

ç‰ˆæœ¬: v2.1 (Bug Fix)
ä¿®å¤:
- ä¿®å¤ ZeroDivisionError: å»¶è¿Ÿåˆ›å»º meterï¼Œé¿å… count=0
- åªåœ¨å®é™…ä½¿ç”¨æ—¶æ‰åˆ›å»ºå’Œæ›´æ–° loss_static å’Œ loss_dynamic

æ–°å¢åŠŸèƒ½:
- æ”¯æŒé™æ€é‡å»ºæŸå¤±ï¼ˆdata_type=0ï¼Œå•å¸§å›¾åƒï¼‰
- æ”¯æŒåŠ¨æ€é‡å»º+é¢„æµ‹æŸå¤±ï¼ˆdata_type=1ï¼Œ4å¸§è§†é¢‘ï¼‰
- éšæœºé€‰æ‹©é™æ€æˆ–åŠ¨æ€æ¨¡å¼è¿›è¡Œè®­ç»ƒ
- åˆ†åˆ«è®°å½•å’Œç›‘æ§ä¸¤ç§æŸå¤±

å‚è€ƒ: AnyTouch - Learning Unified Static-Dynamic Representation
"""

import math
import sys
import random

import torch
import wandb  # â­ æ·»åŠ  Wandb
import util.misc as misc
import util.lr_sched as lr_sched


def train_one_epoch(
    model: torch.nn.Module,  
    data_loader: torch.utils.data.DataLoader,
    optimizer: torch.optim.Optimizer,  
    device: torch.device,   
    epoch: int,   
    loss_scaler,  
    args=None
):  
    """  
    è®­ç»ƒä¸€ä¸ª epochï¼ˆæ”¯æŒé™æ€+åŠ¨æ€é‡å»ºæŸå¤±ï¼‰
    
    å‚æ•°:  
        model: è¦è®­ç»ƒçš„æ¨¡å‹  
        data_loader: æ•°æ®åŠ è½½å™¨ï¼ŒåŠ è½½è¿ç»­å››å¸§è§¦è§‰å›¾ç‰‡
        optimizer: ä¼˜åŒ–å™¨  
        device: è®­ç»ƒè®¾å¤‡ (cuda/cpu)
        epoch: å½“å‰ epoch ç¼–å·  
        loss_scaler: æŸå¤±ç¼©æ”¾å™¨ï¼Œç”¨äºæ··åˆç²¾åº¦è®­ç»ƒ
        args: è®­ç»ƒå‚æ•°é…ç½®ï¼ˆéœ€åŒ…å« static_ratio å‚æ•°ï¼‰
        
    è¿”å›:
        dict: è®­ç»ƒç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """  
    model.train(True)  
    
    # åˆå§‹åŒ– metric logger
    metric_logger = misc.MetricLogger(delimiter="  ")  
    metric_logger.add_meter('lr', misc.SmoothedValue(window_size=1, fmt='{value:.6f}'))
    
    # â­ é‡è¦ï¼šloss_static å’Œ loss_dynamic ä¼šåœ¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶åŠ¨æ€æ·»åŠ 
    # ä¸åœ¨æ­¤å¤„æå‰åˆ›å»ºï¼Œé¿å… ZeroDivisionError (count=0)
    # å»¶è¿Ÿåˆ›å»ºç­–ç•¥ï¼š
    #   - åªåœ¨å®é™…ä½¿ç”¨æ—¶æ‰åˆ›å»º meter
    #   - ç¡®ä¿æ¯ä¸ª meter çš„ count è‡³å°‘ä¸º 1
    #   - é¿å…æ‰“å°æœªä½¿ç”¨çš„ meter æ—¶é™¤é›¶é”™è¯¯
    
    header = 'Epoch: [{}]'.format(epoch)  
    print_freq = 20  

    accum_iter = args.accum_iter  
    optimizer.zero_grad()
    
    # è·å–è®­ç»ƒæ¨¡å¼é…ç½®
    use_joint_training = getattr(args, 'use_joint_training', False)
    static_ratio = getattr(args, 'static_ratio', 0.5)
    
    if use_joint_training:
        print(f'ğŸ¯ Training Mode: Joint Training (è”åˆè®­ç»ƒ)')
        print(f'   Alpha (static weight): {args.alpha}')
        print(f'   Beta (dynamic weight): {args.beta}')
    else:
        print(f'ğŸ² Training Mode: Random Alternating (éšæœºäº¤æ›¿)')
        print(f'   Static ratio: {static_ratio:.1%}')

    # ç»Ÿè®¡é™æ€å’ŒåŠ¨æ€æ¨¡å¼çš„ä½¿ç”¨æ¬¡æ•°
    static_count = 0
    dynamic_count = 0
    joint_count = 0

    # ====================================================================
    # è®­ç»ƒå¾ªç¯
    # ====================================================================
    for data_iter_step, samples in enumerate(metric_logger.log_every(data_loader, print_freq, header)):  

        # å­¦ä¹ ç‡è°ƒåº¦ (æ¯ä¸ªè¿­ä»£è°ƒæ•´ï¼Œè€Œä¸æ˜¯æ¯ä¸ª epoch)
        if data_iter_step % accum_iter == 0:  
            lr_sched.adjust_learning_rate(optimizer, data_iter_step / len(data_loader) + epoch, args)  
          
        # å°†æ•°æ®ç§»åˆ°è®¾å¤‡ä¸Š
        # samples shape: [batch_size, 4, 3, 224, 224]
        samples = samples.to(device, non_blocking=True)  

        # ================================================================
        # â­ æ ¸å¿ƒæ”¹åŠ¨ï¼šè”åˆè®­ç»ƒ vs éšæœºäº¤æ›¿
        # ================================================================
        
        # å‰å‘ä¼ æ’­ï¼ˆä½¿ç”¨æ··åˆç²¾åº¦ï¼‰
        with torch.cuda.amp.autocast():  
            if use_joint_training:
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # ğŸ¯ è”åˆè®­ç»ƒæ¨¡å¼ï¼šåŒæ—¶è®¡ç®—é™æ€å’ŒåŠ¨æ€loss
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                loss_static, loss_dynamic, loss_recon, loss_pred, pred, mask = model(samples, data_type='joint')
                
                # æ£€æŸ¥lossæœ‰æ•ˆæ€§
                if not math.isfinite(loss_static.item()):
                    print(f"âš ï¸ Warning: loss_static is {loss_static.item()}, skipping this batch")
                    continue
                
                if not math.isfinite(loss_dynamic.item()):
                    print(f"âš ï¸ Warning: loss_dynamic is {loss_dynamic.item()}, skipping this batch")
                    continue
                
                # åŠ æƒç»„åˆ
                alpha = args.alpha
                beta = args.beta
                loss = alpha * loss_static + beta * loss_dynamic
                
                # è®°å½•è¯¦ç»†ä¿¡æ¯
                data_type_name = 'joint'
                loss_static_value = loss_static.item()
                loss_dynamic_value = loss_dynamic.item()
                loss_recon_value = loss_recon.item()  # â­ æ–°å¢ï¼šé‡å»ºæŸå¤±
                loss_pred_value = loss_pred.item()    # â­ æ–°å¢ï¼šé¢„æµ‹æŸå¤±
                joint_count += 1
                
            else:
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # ğŸ² éšæœºäº¤æ›¿æ¨¡å¼ï¼šæ¯æ¬¡åªè®¡ç®—ä¸€ç§loss
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                use_static = random.random() < static_ratio
                
                if use_static:
                    # é™æ€æ¨¡å¼ï¼šåªä½¿ç”¨ç¬¬1å¸§é‡å»º
                    loss, _, _ = model(samples[:, 0], data_type=0)
                    data_type_name = 'static'
                    static_count += 1
                else:
                    # åŠ¨æ€æ¨¡å¼ï¼šä½¿ç”¨4å¸§é‡å»º+é¢„æµ‹
                    loss, _, _ = model(samples, data_type=1)
                    data_type_name = 'dynamic'
                    dynamic_count += 1

        loss_value = loss.item()  

        # æ£€æŸ¥æŸå¤±æ˜¯å¦æœ‰æ•ˆï¼ˆé¿å… NaN æˆ– Infï¼‰
        if not math.isfinite(loss_value):  
            print(f"Loss is {loss_value} (mode: {data_type_name}), stopping training")
            sys.exit(1)  

        # æ¢¯åº¦ç´¯ç§¯
        loss /= accum_iter  
        loss_scaler(
            loss, optimizer, parameters=model.parameters(),
            update_grad=(data_iter_step + 1) % accum_iter == 0
        )  
        
        if (data_iter_step + 1) % accum_iter == 0:  
            optimizer.zero_grad()  

        torch.cuda.synchronize()  

        # ================================================================
        # æ›´æ–°æŒ‡æ ‡ï¼ˆæ€»æŸå¤±ï¼‰
        # ================================================================
        metric_logger.update(loss=loss_value)
        
        # ================================================================
        # â­ å»¶è¿Ÿåˆ›å»ºå’Œæ›´æ–°é™æ€/åŠ¨æ€æŸå¤± meter
        # ================================================================
        if use_joint_training:
            # ğŸ¯ è”åˆè®­ç»ƒæ¨¡å¼ï¼šè®°å½•è¯¦ç»†çš„lossåˆ†è§£
            if 'loss_static' not in metric_logger.meters:
                metric_logger.add_meter('loss_static', misc.SmoothedValue(window_size=20, fmt='{value:.4f}'))
            if 'loss_dynamic' not in metric_logger.meters:
                metric_logger.add_meter('loss_dynamic', misc.SmoothedValue(window_size=20, fmt='{value:.4f}'))
            if 'loss_ratio' not in metric_logger.meters:
                metric_logger.add_meter('loss_ratio', misc.SmoothedValue(window_size=20, fmt='{value:.4f}'))
            if 'weighted_static' not in metric_logger.meters:
                metric_logger.add_meter('weighted_static', misc.SmoothedValue(window_size=20, fmt='{value:.4f}'))
            if 'weighted_dynamic' not in metric_logger.meters:
                metric_logger.add_meter('weighted_dynamic', misc.SmoothedValue(window_size=20, fmt='{value:.4f}'))
            
            metric_logger.update(loss_static=loss_static_value)
            metric_logger.update(loss_dynamic=loss_dynamic_value)
            metric_logger.update(loss_ratio=loss_static_value / (loss_dynamic_value + 1e-8))
            metric_logger.update(weighted_static=alpha * loss_static_value)
            metric_logger.update(weighted_dynamic=beta * loss_dynamic_value)
            
        else:
            # ğŸ² éšæœºäº¤æ›¿æ¨¡å¼ï¼šåŸæœ‰é€»è¾‘
            if use_static:
                if 'loss_static' not in metric_logger.meters:
                    metric_logger.add_meter('loss_static', misc.SmoothedValue(window_size=20, fmt='{value:.4f}'))
                metric_logger.update(loss_static=loss_value)
            else:
                if 'loss_dynamic' not in metric_logger.meters:
                    metric_logger.add_meter('loss_dynamic', misc.SmoothedValue(window_size=20, fmt='{value:.4f}'))
                metric_logger.update(loss_dynamic=loss_value)
            
        # æ›´æ–°å­¦ä¹ ç‡
        lr = optimizer.param_groups[0]["lr"]  
        metric_logger.update(lr=lr)  

        loss_value_reduce = misc.all_reduce_mean(loss_value)
        
        # ================================================================
        # â­ Wandb è®°å½•ï¼šæ¯ä¸ª batch ä¸Šä¼ ä¸€æ¬¡ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
        # ================================================================
        if misc.is_main_process() and (data_iter_step + 1) % accum_iter == 0:
            # è®¡ç®—å…¨å±€ iteration
            global_step = data_iter_step // accum_iter + epoch * len(data_loader) // accum_iter
            
            # åŸºç¡€ä¿¡æ¯
            wandb_log = {
                "batch/loss_total": loss_value_reduce,
                "batch/lr": lr,
                "batch/epoch": epoch,
                "batch/iteration": global_step,
            }
            
            # â­ è”åˆè®­ç»ƒ vs éšæœºäº¤æ›¿çš„ä¸åŒè®°å½•æ–¹å¼
            if use_joint_training:
                # ğŸ¯ è”åˆè®­ç»ƒï¼šè®°å½•è¯¦ç»†çš„lossåˆ†è§£å’Œæƒé‡ä¿¡æ¯
                wandb_log.update({
                    "batch/loss_static": loss_static_value,
                    "batch/loss_dynamic": loss_dynamic_value,
                    "batch/loss_recon": loss_recon_value,      # â­ æ–°å¢ï¼šåŠ¨æ€é‡å»ºæŸå¤±(å‰3å¸§)
                    "batch/loss_pred": loss_pred_value,        # â­ æ–°å¢ï¼šé¢„æµ‹æŸå¤±(ç¬¬4å¸§)
                    "batch/loss_ratio": loss_static_value / (loss_dynamic_value + 1e-8),
                    "batch/weighted_static": alpha * loss_static_value,
                    "batch/weighted_dynamic": beta * loss_dynamic_value,
                    "batch/alpha": alpha,
                    "batch/beta": beta,
                    "batch/static_contribution": (alpha * loss_static_value) / loss_value_reduce,
                    "batch/dynamic_contribution": (beta * loss_dynamic_value) / loss_value_reduce,
                    "batch/mode": 2,  # è”åˆæ¨¡å¼
                })
            else:
                # ğŸ² éšæœºäº¤æ›¿ï¼šè®°å½•å½“å‰batchçš„æ¨¡å¼
                if use_static:
                    wandb_log["batch/loss_static"] = loss_value_reduce
                    wandb_log["batch/mode"] = 0  # é™æ€æ¨¡å¼
                else:
                    wandb_log["batch/loss_dynamic"] = loss_value_reduce
                    wandb_log["batch/mode"] = 1  # åŠ¨æ€æ¨¡å¼
            
            # ä¸Šä¼ åˆ° Wandb
            wandb.log(wandb_log)

    # ====================================================================
    # Epoch ç»“æŸåçš„ç»Ÿè®¡
    # ====================================================================
    
    # åŒæ­¥æ‰€æœ‰è¿›ç¨‹çš„ç»Ÿè®¡ä¿¡æ¯
    metric_logger.synchronize_between_processes()  
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("Averaged stats:", metric_logger)
    
    # æ‰“å°æ¨¡å¼ä½¿ç”¨ç»Ÿè®¡
    if use_joint_training:
        print(f"ğŸ¯ Joint training: {joint_count} batches")
        if joint_count > 0 and 'loss_static' in metric_logger.meters and 'loss_dynamic' in metric_logger.meters:
            print(f"   Avg loss_static: {metric_logger.meters['loss_static'].global_avg:.4f}")
            print(f"   Avg loss_dynamic: {metric_logger.meters['loss_dynamic'].global_avg:.4f}")
            print(f"   Avg loss_ratio: {metric_logger.meters['loss_ratio'].global_avg:.4f}")
    else:
        total_count = static_count + dynamic_count
        if total_count > 0:
            print(f"ğŸ² Static mode: {static_count} batches ({static_count/total_count:.1%})")
            print(f"ğŸ² Dynamic mode: {dynamic_count} batches ({dynamic_count/total_count:.1%})")
        else:
            print("âš ï¸ Warning: No batches processed!")
    
    # è¿”å›ç»Ÿè®¡ä¿¡æ¯
    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}
