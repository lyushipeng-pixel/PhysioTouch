"""
main_stage1_static.py - æ”¯æŒé™æ€é‡å»ºæŸå¤±çš„ä¸»è®­ç»ƒè„šæœ¬

ç‰ˆæœ¬: v2.0
æ–°å¢åŠŸèƒ½:
- æ”¯æŒé™æ€é‡å»ºæŸå¤±ï¼ˆå•å¸§å›¾åƒé‡å»ºï¼‰
- æ”¯æŒåŠ¨æ€é‡å»º+é¢„æµ‹æŸå¤±ï¼ˆ4å¸§è§†é¢‘ï¼‰
- é€šè¿‡ --static_ratio å‚æ•°æ§åˆ¶é™æ€/åŠ¨æ€æ¯”ä¾‹

å‚è€ƒ: AnyTouch - Learning Unified Static-Dynamic Representation
"""

import os
import torch
from transformers import AutoConfig
from transformers.models.vit.configuration_vit import ViTConfig
from model.mae_model import TactileVideoMAE
from config import parse_args
import random
import numpy as np
from dataloader.stage1_dataset import PretrainDataset_Contact
import torch.backends.cudnn as cudnn
# from torch.utils.tensorboard import SummaryWriter  # å·²åˆ é™¤TensorBoard
import timm.optim.optim_factory as optim_factory
from Stage1_engine import train_one_epoch  # â­ ä½¿ç”¨æ–°çš„è®­ç»ƒå¼•æ“
import datetime
import json
import time
from pathlib import Path
import copy
import wandb  # â­ æ·»åŠ  Wandb

import util.misc as misc
from util.misc import NativeScalerWithGradNormCount as NativeScaler

torch.cuda.device_count.cache_clear()


def load_model_from_clip_video(ckpt, model):  
    """ä» CLIP é¢„è®­ç»ƒæƒé‡åˆå§‹åŒ–è§¦è§‰æ¨¡å‹"""  
    new_ckpt = {}  
    for key, item in ckpt.items():  
        if "vision_model" in key and 'position_ids' not in key:  
            new_key = key.replace("vision_model", "touch_model")  
            new_ckpt[new_key] = item  
      
    msg = model.load_state_dict(new_ckpt, strict=False)  
    print(f"Loaded CLIP weights: {msg}")  
    return model 


def main(args):
    misc.init_distributed_mode(args)
    print('job dir: {}'.format(os.path.dirname(os.path.realpath(__file__))))  
    print("{}".format(args).replace(', ', ',\n'))

    device = torch.device(args.device)

    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
    seed = args.seed + misc.get_rank()
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)  
    cudnn.benchmark = True  

    # â­ æ‰“å°è®­ç»ƒé…ç½®
    print(f"\n{'='*70}")
    print(f"ğŸ¯ Training Configuration:")
    print(f"{'='*70}")
    if args.use_joint_training:
        print(f"  Training Mode: ğŸ¯ Joint Training (è”åˆè®­ç»ƒ)")
        print(f"  â”œâ”€ Alpha (static weight):  {args.alpha}")
        print(f"  â”œâ”€ Beta (dynamic weight):  {args.beta}")
        print(f"  â””â”€ Expected contribution:")
        total_weight = args.alpha + args.beta
        print(f"     â”œâ”€ Static:  {args.alpha/total_weight:.1%} of total loss")
        print(f"     â””â”€ Dynamic: {args.beta/total_weight:.1%} of total loss")
    else:
        print(f"  Training Mode: ğŸ² Random Alternating (éšæœºäº¤æ›¿)")
        print(f"  â”œâ”€ Static ratio:  {args.static_ratio:.2%}")
        print(f"  â””â”€ Dynamic ratio: {1-args.static_ratio:.2%}")
    print(f"  Mask ratio: {args.mask_ratio:.1%}")
    print(f"  Gradient monitoring: {'âœ… Enabled' if args.monitor_gradient else 'âŒ Disabled'}")
    print(f"{'='*70}\n")

    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    dataset_train = PretrainDataset_Contact(mode='train')
    print(f'Dataset size: {len(dataset_train)}')
    
    num_tasks = misc.get_world_size()
    global_rank = misc.get_rank()
    sampler_train = torch.utils.data.DistributedSampler(
        dataset_train, num_replicas=num_tasks, rank=global_rank, shuffle=True
    )
    print(f"Sampler = {sampler_train}") 

    data_loader_train = torch.utils.data.DataLoader(
        dataset_train, sampler=sampler_train,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        pin_memory=True,
        drop_last=True,
    )

    # æ¨¡å‹åˆå§‹åŒ–
    config = AutoConfig.from_pretrained('CLIP-ViT-L-14-DataComp.XL-s13B-b90K/config.json')
    
    # é…ç½®è§£ç å™¨
    decoder_config = ViTConfig()
    decoder_config.encoder_stride = 14
    decoder_config.hidden_size = 512
    decoder_config.intermediate_size = 2048
    decoder_config.num_attention_heads = 16
    decoder_config.num_hidden_layers = 8
    decoder_config.patch_size = 14

    # åˆ›å»ºæ¨¡å‹
    model = TactileVideoMAE(args, config, decoder_config, 1, False, 1)
    model.initialize_decoder()
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    ckpt = torch.load('CLIP-ViT-L-14-DataComp.XL-s13B-b90K/pytorch_model.bin', map_location='cpu')
    model = load_model_from_clip_video(ckpt, model)
    model.to(device)

    model_without_ddp = model
    print("Model = %s" % str(model_without_ddp))

    # è®¡ç®—æœ‰æ•ˆæ‰¹æ¬¡å¤§å°å’Œå­¦ä¹ ç‡
    eff_batch_size = args.batch_size * args.accum_iter * misc.get_world_size()
    
    if args.lr is None:
        args.lr = args.blr * eff_batch_size / 256

    print("base lr: %.2e" % (args.lr * 256 / eff_batch_size))
    print("actual lr: %.2e" % args.lr)
    print("accumulate grad iterations: %d" % args.accum_iter)
    print("effective batch size: %d" % eff_batch_size)

    # åˆ†å¸ƒå¼è®­ç»ƒåŒ…è£…
    if args.distributed:
        model = torch.nn.parallel.DistributedDataParallel(
            model, device_ids=[args.gpu], find_unused_parameters=True
        )
        model_without_ddp = model.module

    # ä¼˜åŒ–å™¨è®¾ç½®
    param_groups = optim_factory.param_groups_weight_decay(model_without_ddp, args.weight_decay)
    optimizer = torch.optim.AdamW(
        param_groups, lr=args.lr, weight_decay=args.weight_decay, betas=(0.9, 0.99)
    )
    print(optimizer)
    loss_scaler = NativeScaler()

    # åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    misc.load_model(
        args=args, model_without_ddp=model_without_ddp, 
        optimizer=optimizer, loss_scaler=loss_scaler
    )

    print(f"Start training for {args.epochs} epochs")
    
    # æ‰“å°è®­ç»ƒæ¨¡å¼ä¿¡æ¯
    if args.use_joint_training:
        print(f"ğŸ¯ Joint Training Mode - Alpha: {args.alpha}, Beta: {args.beta}")
    else:
        print(f"ğŸ² Random Alternating Mode - Static ratio: {args.static_ratio}")

    # â­ åˆå§‹åŒ– Wandbï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
    if misc.is_main_process():
        # ç”ŸæˆåŸºäºæ—¶é—´çš„ run name
        run_name = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # åˆå§‹åŒ– wandb
        wandb_config = {
            # åŸºç¡€è®­ç»ƒé…ç½®
            "batch_size": args.batch_size,
            "epochs": args.epochs,
            "learning_rate": args.lr,
            "warmup_epochs": args.warmup_epochs,
            "accum_iter": args.accum_iter,
            "weight_decay": args.weight_decay,
            "effective_batch_size": eff_batch_size,
            
            # MAEé…ç½®
            "mask_ratio": args.mask_ratio,
            "norm_pix_loss": args.norm_pix_loss,
            "use_video": args.use_video,
            
            # è®­ç»ƒæ¨¡å¼é…ç½®
            "use_joint_training": args.use_joint_training,
            "monitor_gradient": args.monitor_gradient,
        }
        
        # æ ¹æ®è®­ç»ƒæ¨¡å¼æ·»åŠ ç‰¹å®šé…ç½®
        if args.use_joint_training:
            wandb_config.update({
                "training_mode": "joint",
                "alpha": args.alpha,
                "beta": args.beta,
                "expected_static_contribution": args.alpha / (args.alpha + args.beta),
                "expected_dynamic_contribution": args.beta / (args.alpha + args.beta),
            })
        else:
            wandb_config.update({
                "training_mode": "alternating",
                "static_ratio": args.static_ratio,
            })
        
        wandb.init(
            project="PhysioTouch-Stage1",
            name=run_name,
            config=wandb_config,
            tags=["joint_training" if args.use_joint_training else "alternating"],
        )
        
        # â­ å®šä¹‰æŒ‡æ ‡çš„é»˜è®¤æ˜¾ç¤ºè¡Œä¸ºï¼ˆä¼˜åŒ–Wandb UIï¼‰
        # è®¾ç½®xè½´ä¸ºiterationï¼ˆå…¨å±€æ­¥æ•°ï¼‰
        wandb.define_metric("batch/iteration")
        wandb.define_metric("batch/*", step_metric="batch/iteration")
        
        # è®¾ç½®epochç›¸å…³æŒ‡æ ‡çš„xè½´ä¸ºepoch
        wandb.define_metric("epoch", step_metric="epoch")
        wandb.define_metric("epoch/*", step_metric="epoch")
        
        # â­ ä¸ºå…³é”®æŒ‡æ ‡è®¾ç½®ç›®æ ‡ï¼ˆå¸®åŠ©Wandbè‡ªåŠ¨è¯†åˆ«é‡è¦æ€§ï¼‰
        if args.use_joint_training:
            # è”åˆè®­ç»ƒæ¨¡å¼çš„å…³é”®æŒ‡æ ‡
            wandb.define_metric("batch/loss_total", summary="min")  # æœ€å°åŒ–æ€»æŸå¤±
            wandb.define_metric("batch/loss_static", summary="min")
            wandb.define_metric("batch/loss_dynamic", summary="min")
            wandb.define_metric("batch/loss_ratio", summary="mean")  # å¹³å‡å€¼
            wandb.define_metric("batch/static_contribution", summary="mean")  # âš ï¸ ç§»é™¤äº†ä¸æ”¯æŒçš„goalå‚æ•°
            wandb.define_metric("epoch/train_loss", summary="min")
            wandb.define_metric("epoch/best_loss_so_far", summary="min")
            
            print("âœ… Wandb metrics configured for joint training mode")
        else:
            wandb.define_metric("batch/loss_total", summary="min")
            print("âœ… Wandb metrics configured for alternating mode")
        print(f"âœ… Wandb initialized: Project=PhysioTouch-Stage1, Run={run_name}")
    
    # â­ ä¸ä½¿ç”¨ TensorBoard
    log_writer = None

    # â­ åˆå§‹åŒ–æœ€ä½³æŸå¤±è¿½è¸ª
    best_loss = float('inf')
    best_epoch = -1

    # è®­ç»ƒå¾ªç¯
    start_time = time.time()
    for epoch in range(args.start_epoch, args.epochs):
        sampler_train.set_epoch(epoch)  
        
        # â­ ä½¿ç”¨æ”¯æŒé™æ€é‡å»ºçš„è®­ç»ƒå‡½æ•°
        train_stats = train_one_epoch(  
            model=model,  
            data_loader=data_loader_train,
            optimizer=optimizer,  
            device=device,  
            epoch=epoch,  
            loss_scaler=loss_scaler,  
            args=args  # åŒ…å« static_ratio å‚æ•°
        ) 
        
        # â­ æ–°çš„ä¿å­˜ç­–ç•¥ï¼šä¿å­˜æœ€ä½³(best)å’Œæœ€å(last)
        if args.output_dir and misc.is_main_process():
            # è·å–å½“å‰epochçš„è®­ç»ƒæŸå¤±
            current_loss = train_stats.get('loss', float('inf'))
            
            # 1. å§‹ç»ˆä¿å­˜/æ›´æ–° last.pthï¼ˆæœ€åä¸€æ¬¡è®­ç»ƒç»“æœï¼‰
            last_checkpoint_path = os.path.join(args.output_dir, 'last.pth')
            torch.save({
                'model': model_without_ddp.state_dict(),
                'optimizer': optimizer.state_dict(),
                'epoch': epoch,
                'scaler': loss_scaler.state_dict(),
                'args': args,
                'loss': current_loss,
            }, last_checkpoint_path)
            print(f'[Epoch {epoch}] Saved last.pth (loss: {current_loss:.4f})')
            
            # 2. å¦‚æœæ˜¯æœ€ä½³ç»“æœï¼Œä¿å­˜/æ›´æ–° best.pth
            if current_loss < best_loss:
                best_loss = current_loss
                best_epoch = epoch
                best_checkpoint_path = os.path.join(args.output_dir, 'best.pth')
                torch.save({
                    'model': model_without_ddp.state_dict(),
                    'optimizer': optimizer.state_dict(),
                    'epoch': epoch,
                    'scaler': loss_scaler.state_dict(),
                    'args': args,
                    'loss': current_loss,
                }, best_checkpoint_path)
                print(f'â­ [Epoch {epoch}] New best model! Saved best.pth (loss: {current_loss:.4f})')  

        # è®°å½•æ—¥å¿—
        log_stats = {
            **{f'train_{k}': v for k, v in train_stats.items()},
            'epoch': epoch,
        }

        if args.output_dir and misc.is_main_process():
            # è®°å½•åˆ°æ–‡æœ¬æ—¥å¿—
            with open(os.path.join(args.output_dir, "log.txt"), mode="a", encoding="utf-8") as f:
                f.write(json.dumps(log_stats) + "\n")
            
            # â­ è®°å½• epoch çº§åˆ«çš„æ±‡æ€»ä¿¡æ¯åˆ° Wandb
            epoch_log = {
                "epoch": epoch,
                "epoch/train_loss": train_stats.get('loss', 0),
                "epoch/lr": train_stats.get('lr', 0),
                "epoch/best_loss_so_far": best_loss,
            }
            
            # å¦‚æœæ˜¯è”åˆè®­ç»ƒï¼Œè®°å½•æ›´è¯¦ç»†çš„lossåˆ†è§£
            if args.use_joint_training:
                epoch_log.update({
                    "epoch/loss_static": train_stats.get('loss_static', 0),
                    "epoch/loss_dynamic": train_stats.get('loss_dynamic', 0),
                    "epoch/loss_ratio": train_stats.get('loss_ratio', 0),
                    "epoch/weighted_static": train_stats.get('weighted_static', 0),
                    "epoch/weighted_dynamic": train_stats.get('weighted_dynamic', 0),
                })
                
                # è®¡ç®—å®é™…çš„contributionæ¯”ä¾‹
                loss_static = train_stats.get('loss_static', 0)
                loss_dynamic = train_stats.get('loss_dynamic', 0)
                if loss_static > 0 and loss_dynamic > 0:
                    epoch_log.update({
                        "epoch/actual_static_contribution": (args.alpha * loss_static) / (args.alpha * loss_static + args.beta * loss_dynamic),
                        "epoch/actual_dynamic_contribution": (args.beta * loss_dynamic) / (args.alpha * loss_static + args.beta * loss_dynamic),
                    })
            
            wandb.log(epoch_log)

    total_time = time.time() - start_time
    total_time_str = str(datetime.timedelta(seconds=int(total_time)))
    print('Training time {}'.format(total_time_str))
    
    # â­ æ‰“å°æœ€ä½³æ¨¡å‹ä¿¡æ¯
    if misc.is_main_process():
        print('\n' + '='*80)
        print('Training Summary:')
        print(f'  Best model at epoch {best_epoch} with loss: {best_loss:.4f}')
        print(f'  Saved models:')
        print(f'    - best.pth  (epoch {best_epoch}, loss {best_loss:.4f})')
        print(f'    - last.pth  (epoch {args.epochs-1})')
        print('='*80)
        
        # â­ è®°å½•æœ€ç»ˆç»Ÿè®¡åˆ° Wandb
        wandb.log({
            "summary/best_epoch": best_epoch,
            "summary/best_loss": best_loss,
            "summary/total_time_seconds": total_time,
        })
        
        # â­ ç»“æŸ Wandb è¿½è¸ª
        wandb.finish()
        print("âœ… Wandb logging finished")


if __name__ == "__main__":  
    parser = parse_args()  
    args = parser.parse_args()  
      
    if args.output_dir:  
        Path(args.output_dir).mkdir(parents=True, exist_ok=True)  
      
    main(args)


